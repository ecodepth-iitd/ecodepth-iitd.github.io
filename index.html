
<!-- saved from url=(0035)https://diffusion-vision.github.io/ -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <!-- from MDCA -->
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <meta name="author" content="Suraj Patni">
    <meta name="description"
        content="Project page of Effective Conditioning of Diffusion Models for Monocular Depth Estimation">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <link rel="icon" type="image/png" href="cvpr.png">
    <!-- Format -->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="../format/app.css">
    <link rel="stylesheet" href="../format/bootstrap.min.css">
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>
    <script src="../format/app.js"></script>

    <title>ECoDepth - Effective Conditioning of Diffusion Models for Monocular Depth Estimation</title>
    <!-- <link rel="icon" href="iitd_logo.png" type="image/png"> -->
    <link rel="icon" href="assets/CVPR-logo.svg" type="image/png">
    <meta name="viewport" content="width=device-width, user-scalable=no, minimum-scale=1.0, maximum-scale=1.0">
    <link type="text/css" rel="stylesheet" href="./ecodepth/main.css">
    <link rel="preconnect" href="https://fonts.googleapis.com/">
    <link rel="preconnect" href="https://fonts.gstatic.com/" crossorigin="">
    <link rel="stylesheet" href="./ecodepth/css2">
    <style>
        .title {
          color: black; /* Set default color */
        }
      
        .gray-text {
          color: #828282; /* Use your desired shade of grey */
          /* Bold*/
          font-weight: bold;
        }
        .gray-text2 {
          color: #515151; /* Use your desired shade of grey */
          /* Bold*/
          font-weight: bold;
        }
        .author {
            color: #088df3; /* Use your desired shade of grey */
        }
        .affiliations2 {
            color: #7d7d7d; /* Use your desired shade of grey */
            text-align: center;
            font-size: smaller;
        }
        .affiliations {
            font-size: larger;
        }
        .logo {
            position: fixed;
        }
        .logo img {
            position: fixed;
            width: 90px; /* Adjust the width as needed */
            height: auto; /* Maintain aspect ratio */
            top: 10px; /* Adjust as needed */
            right: 10px; /* Adjust as needed */
            z-index: 999; /* Ensure the logo appears on top of other content */
            
        }
        .center {
            text-align: center;
        }
        .bigger-font {
            font-size: 24px; /* Adjust the font size as needed */
        }
        .bold-text {
            font-weight: bold;
        }
        .left-align {
            text-align: left;
        }
        .font-size-author-names {
            font-size: 21px;
        }
        .center-align {
            text-align: center;
        }
        .section{
            font-size: 150%;
            font-weight: 500;
            /* background: rgba(0,0,0,0.03); */
            padding-top: 0.5em;
            padding-bottom: 0.5em;
            color: #565656;
            text-align: center;
            /* padding-left: 0.5em; */
        }
      </style>
    </head>
    
<body data-new-gr-c-s-check-loaded="14.1162.0" data-gr-ext-installed="">
    <div class="container">
    <!-- <div class="BbxBP a3ETed K5Zlne" jsname="WA9qLc" jscontroller="RQOkef" jsaction="rcuQ6b:ywL4Jf;VbOlFf:ywL4Jf;FaOgy:ywL4Jf; keydown:Hq2uPe; wheel:Ut4Ahc;" data-top-navigation="true" data-is-preview="true"></div> -->
    <div class="logo">
        <img src="assets/cvpr_logo.jpeg" alt="Logo">
    </div>
    <p class="title"><span class="gray-text">ECoDepth</span><br>Effective Conditioning of Diffusion Models for Monocular Depth Estimation</p>
    <p class="gray-text2 center bigger-font">CVPR 2024</p>
    <!-- <p class="title">Effective Conditioning of Diffusion Models for Monocular Depth Estimation</p> -->

    <p class="author">
        <span class="author font-size-author-names">
            <a href="https://www.linkedin.com/in/suraj-patni">
            Suraj&nbsp;Patni*</a>
        </span>
        <span class="author font-size-author-names">
            <a href="https://www.linkedin.com/in/aradhye-agarwal-a545a4218/">
                Aradhye&nbsp;Agarwal*</a>
        </span>
        <span class="author font-size-author-names">
            <a href="https://www.cse.iitd.ac.in/~chetan">
                Chetan&nbsp;Arora</a>
        </span>
    </p>
    <div class="affiliations">
        <!-- <a href="https://home.iitd.ac.in/"> -->
            Indian Institute of Technology Delhi
        <!-- </a> -->
    </div>
    
    <div class="row">
        <div class="col-md-8 col-md-offset-2 text-center">
            <ul class="nav nav-pills nav-justified">
                <li>
                    <a href="https://arxiv.org/abs/2403.18807">
                        <img src="assets/paper_front.png" height="80px"><br>
                        <h4><strong>Paper</strong></h4>
                    </a>
                </li>
                <li>
                    <a href="https://arxiv.org/abs/2403.18807">
                        <img src="assets/supplementary_front.png" height="80px"><br>
                        <h4><strong>Supplementary</strong></h4>
                    </a>
                </li>
                <li>
                    <a href="https://github.com/Aradhye2002/EcoDepth">
                        <img src="assets/github_icon.png" height="80px"><br>
                            <h4><strong>Code</strong></h4>
                        </a>
                </li>
                <li>
                    <a href="https://arxiv.org/abs/2403.18807">
                        <img src="assets/arxiv.png" height="80px"><br>
                        <h4><strong>arXiv</strong></h4>
                    </a>
                </li>
            </ul>
        </div>
    </div>
    <p class="section center">Abstract</p>
    <p>
        In the absence of parallax cues, a learning based single image depth estimation (SIDE) model relies heavily on shading and contextual cues in the image. While this simplicity is attractive, it is necessary to train such models on large and varied datasets, which are difficult to capture. It has been shown that using embeddings from pretrained foundational models, such as CLIP, improves zero shot transfer in several applications. Taking inspiration from this, in our paper we explore the use of global image priors generated from a pre-trained ViT model to provide more detailed contextual information. We argue that the embedding vector from a ViT model, pre-trained on a large dataset, captures greater relevant information for SIDE than the usual route of generating pseudo image captions, followed by CLIP based text embeddings. Based on the idea, we propose a new SIDE model using a diffusion backbone conditioned on ViT embeddings. Our proposed design establishes a new state-of-the-art (SOTA) for SIDE on NYU Depth v2 dataset, achieving Abs Rel error of 0.059(14% improvement) compared to 0.069 by the current SOTA (VPD). And on KITTI dataset, achieving SqRel error of 0.139 (2% improvement) compared to 0.142 by the current SOTA (GEDepth). For zero shot transfer with a model trained on NYU Depth v2, we report mean relative improvement of (20%, 23%, 81%, 25%) over NeWCRF on (Sun-RGBD, iBims1, DIODE, HyperSim) datasets, compared to (16%, 18%, 45%, 9%) by ZoEDepth.
    </p>
    

    <div class="flex-container margin-top-1em margin-bottom-1em">
        <figure>
        <img src="./assets/aarch_diagram.png">
        <figcaption class="left-align"><span class="bold-text">Architecture Diagram: </span>The latent representations of the input image undergo a diffusion process, which is conditioned by our proposed CIDE module. Within the CIDE module, the input image is fed through the frozen ViT model. From this, a linear combination of learnable embeddings is computed, which are subsequently transformed to generate a 768-dimensional semantic context embedding. This embedding is utilized to condition the diffusion backbone. Subsequently, hierarchical feature maps are extracted from U-Net's decoder which are upsampled and processed through a depth regressor to generate the depth map.</figcaption>
        </figure>
    </div>  



    <p class="section center-align">Demo ZeroShot Performance on Video</p>
    <div class="flex-container margin-top-1em margin-bottom-1em">
        <figure class="center-align">
            <div id="video-container">
                <iframe id="video-iframe" src="./assets/demo_zeroshot_video.mp4" 
                        title="YouTube video player" 
                        allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" 
                        allowfullscreen>
                </iframe>
            </div>
            <figcaption class="left-align">
                <span class="bold-text">Demo Video: </span>
                Our depth estimation model in action, capturing intricate details of the driving scene with precision.
            </figcaption>
        </figure>
    </div>
    <script>
        window.onload = function() {
            var videoIframe = document.getElementById('video-iframe');
            var videoContainer = document.getElementById('video-container');
            
            videoIframe.onload = function() {
                // var aspectRatio = this.width / this.height;
                var aspectRatio = 1562/720;
                var containerWidth = videoContainer.offsetWidth;
                var containerHeight = containerWidth / aspectRatio;
                videoIframe.style.width = containerWidth + 'px';
                videoIframe.style.height = containerHeight + 'px';
            }
        };
    </script>


    <p class="section">State-of-the-art results on Monocular Depth Estimation</p>
    Our model achieves state-of-the-art results on the indoor dataset NYUv2 and outdoor dataset KITTI for metric depth estimation using monocular images.
    <!-- <div class="flex-container margin-top-1em margin-bottom-1em"> -->
    <div class="flex-container margin-top-1em margin-bottom-1em">
        <figure class="less_width">
          <img src="./assets/table_nyu.png">
          <figcaption>Table 1: NYUv2 Dataset </figcaption>
        </figure>
    </div>
    
    <div class="flex-container margin-top-1em margin-bottom-1em">
        <figure class="less_width">
          <img src="./assets/table_kitti.png">
          <figcaption>Table 2: KITTI Dataset</figcaption>
        </figure>
    </div>
    <!-- </div> -->
    <p class="section">Generalization and Zero Shot Transfer</p>
    Our model generalizes well to other datasets and achieves state-of-the-art results for zero-shot transfer on various datasets. Our model which is trained only on NYUv2 dataset is tested on the following datasets: Sun-RGBD, iBims1, DIODE, and HyperSim. Achieving SOTA results compared to all previous methods that claim to generalize on unseen data(ex: ZoeDepth).
    <div class="flex-container margin-top-1em margin-bottom-1em">
        <figure class="fullwidth">
          <img src="./assets/table_zeroshot.png">
          <figcaption></figcaption>
        </figure>
    </div>
    <p class="section text-center">Zero-Shot Qualitative Results</p>
    <div class="flex-container margin-top-1em margin-bottom-1em">
        <figure class="less_width">
          <img src="./assets/teaser.png">
          <figcaption  class="left-align"><span class="bold-text">Qualitative results</span> across four different datasets, demonstrating the zero-shot performance of our model trained only on the NYUv2 dataset. Corresponding quantitative results are presented in above table. The first column displays RGB images, the second column depicts ground truth depth, and the third column showcases our model's predicted depths. Additional images for each dataset are available in the Supplementary Material.</figcaption>
        </figure>
    </div>

<p class="section">BibTeX (Citation)</p>
<pre class="selectable"><code>
@article{patni2024ecodepth,
  title={ECoDepth: Effective Conditioning of Diffusion Models for Monocular Depth Estimation},
  author={Patni, Suraj and Agarwal, Aradhye and Arora, Chetan},
  journal={arXiv preprint arXiv:2403.18807},
  year={2024}
}
</code></pre>
    
</div>
</body>
</html>
